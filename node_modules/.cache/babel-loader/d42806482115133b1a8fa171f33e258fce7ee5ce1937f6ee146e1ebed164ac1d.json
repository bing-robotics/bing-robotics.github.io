{"ast":null,"code":"var _jsxFileName = \"D:\\\\Binghamton\\\\bing-robotics.github.io\\\\src\\\\App.js\";\n// import React from 'react';\n// import './App.css';\n// import logo from './assets/logo1.jpeg';\n\n// function App() {\n//     return (\n//         <div className=\"App\">\n//             <header className=\"App-header\">\n//                 <img src={logo} className=\"App-logo\" alt=\"Binghamton University Logo\" />\n//                 <h1>Binghamton Robotics Seminar Talks</h1>\n//             </header>\n//             <header className=\"App-navbar\">\n//                 <nav>\n//                     <ul>\n//                         <li><a href=\"#upcoming-talks\">UPCOMING TALKS</a></li>\n//                         <li><a href=\"#past-talks\">PAST TALKS</a></li>\n//                         <li><a href=\"#archive\">ARCHIVE</a></li>\n//                     </ul>\n//                 </nav>\n//             </header>\n//             <main>\n//             <section className=\"talks-section\">\n//                     <div className=\"upcoming-talks\" id=\"upcoming-talks\">\n//                         <h2>Upcoming Talks</h2>\n//                         {/* New Talk Entry for Shiming Fang*/}\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p> Wednesday, October 16, 2024, 11:00 AM, EB P03 |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>Shiming Fang</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial   guesses,   labor-intensive  fitting  procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges,   we introduce the Fine-Tuning Hybrid Dynamics   (FTHD)   method,   which   integrates   supervised   and unsupervised   Physics-Informed   Neural   Networks   (PINNs), combining  physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively   manage   noisy   real-world   data,   ensuring   accurate denoising   while   preserving   the   vehicle's   essential   physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale   real-world   experiments   from   the   Indy   Autonomous Challenge.   Results   demonstrate   that   the   hybrid   approach significantly improves parameter estimation accuracy, even with reduced   data,   and   outperforms   existing   models.   EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.</p>\n//                                     <p><strong>Bio:</strong> Shiming Fang received the B.S. degree in Mechanical Engineering from Wuhan University of Technology, Wuhan, China in 2016, and the M.S. degree in Mechanical Engineering from University of Birmingham, Birmingham, UK in 2017. He is currently working towards a Ph.D. degree in Mechanical Engineering at Binghamton University. His current research focuses on autonomous vehicle modeling, robust control, and artificial intelligence in autonomous driving.</p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                         {/* New Talk Entry for Stephanie Jesso*/}\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p> Wednesday, October 23, 2024, 11:00 AM, EB P03 |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>Stephanie Jesso</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Historically, the fields of computer science, cognitive science, and neuroscience have been tightly linked. To date, this collaboration has yielded major advances in how the brain and mind are understood, as well as the ways in which artificial minds can be constructed to serve as new collaborators to humans. Yet there are still significant gaps between the capabilities of state-of-the-art autonomous robots and the expectations developed by real users who are now encountering autonomous robots on the job. Human-centered design and human factors research can help to bridge these gaps to create safe, effective, and desirable human-robot systems.</p>\n//                                     <p>In this talk, I will discuss how human minds compare to current-state AI systems, and how insights from human cognition can be used to create more efficient and capable autonomous robots. I will also present a case study of an evaluation conducted on two autonomous robots intended to aid nurses within hospital settings: Moxi and TUG. Both cobots were originally considered for procurement by our collaborating healthcare system, UHS, at which time our team discovered a lack of evidence on either platform in academic literature. This led us to analyze user comments on social media, which elucidated common struggles within real-world environments. In order to improve the fit of autonomous robots into human environments, collaborative research must be conducted and evidence must be shared. Hope to see you there! </p>\n//                                     <p><strong>Relevant Papers:</strong></p>\n//                                     <p><a href=\"https://www.researchgate.net/publication/380014080_On_the_potential_for_human-centered_cognitively_inspired_AI_to_bridge_the_gap_between_optimism_and_reality_for_autonomous_robotics_in_healthcare_a_respectful_critique\"> Feel free to check out our conference proceedings paper</a>!</p>\n//                                     <p><strong>Bio:</strong> Dr. Stephanie Tulk Jesso is an assistant professor in the School of Systems Science and Industrial Engineering. She is the PI of the Human-Centered Mindful Technologies Lab at Binghamton, a co-founder of the SUNY Nursing, Engineering and Applied Research Collaborative, and focuses on research related to design, implementation, and impacts of emerging technologies in the real world.</p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                     </div>\n//                     <div className=\"past-talks\" id=\"past-talks\">\n//                         <h2>Past Talks</h2>\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p> Friday, September 13, 2024, 12:00 PM, Engineering Building, T1 |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>Zain Nasir</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Mapping and Localization in large environments is becoming increasingly important for autonomous UAV swarms. UAV swarms solving problems in disaster response, infrastructure inspection, and agriculture rely on fresh and accurate maps to make navigation decisions. SLAM methods are capable of providing highly accurate maps through visual information, but are computationally heavy and ill-suited for unmanned aerial vehicles. For this reason, UAV swarms often dedicate one or more drones to frequent mapping, while other drones use map information for planning and trajectory generation. UAV swarms also centralize heavy-weight workloads like AI inference and SLAM map combination at the edge to extend UAV battery lives at the expense of network availability. Both map sharing and offloading necessitate high network bandwidth, but few SLAM or planning approaches account for this. We present NetSLAM, a network-assisted SLAM and planning system that builds environmental maps and UAV trajectories that meet quality of service requirements. NetSLAM embeds network information into SLAM maps so planning can compensate for changes in network connectivity across the environment. We also present NetPlan, a path planning algorithm which utilizes NetSLAM maps to build trajectories that maintain network connectivity requirements to maximize performance. Through real-world experiments and simulation, we show that NetSLAM maps network environments with limited additional overhead compared to existing SLAM approaches, while improving offloading performance significantly when multi-agent swarms consider network availability.</p>\n//                                     <p><strong>Bio:</strong> I am an incoming software developer and researcher at Memorial Sloan Kettering currently completing my Masters in Computer Science (AI Track) from Binghamton University. My expertise includes autonomous navigation systems in drones, computer vision, and machine learning. My current research involves optimizing edge-based SLAM techniques and tailoring them for deployment on small drones with limited compute power.</p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                         <div className=\"talk-notes\">\n//                             <p>This Bing-Robotics talk is also part of the SoC Seminar Series, jointly hosted by Profs. KD Kang and William Hallahan.</p>\n//                         </div>\n//                         {/* New Talk Entry for David DeFazio */}\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p>Friday, August 30, 2024, 12:00 PM, Fountain Room (Smart Energy Building, ITC), Side A |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>David DeFazio</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> Legged Locomotion and Collaborative Decision Making in Human-Robot Teams</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Legged robots are of great interest to the robotics community, due to their capacity for agile movements and robust locomotion in challenging environments. Several methods exist to enable locomotion and navigation capabilities for legged robots, however these methods generally do not leverage human domain knowledge or support direct interactions with people for collaborative decision making. In this talk, I will present a human-in-the-loop approach for both locomotion gait design and guided navigation. Each approach has been deployed on hardware, showcasing diverse locomotion gaits, and guided navigation for the visually impaired.</p>\n//                                     <p><strong>Bio:</strong> </p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                         <div className=\"talk-notes\">\n//                             <p>Prof. Kaiyan Yu will offer a lab tour for all who are interested after the seminar talk.</p>\n//                         </div>\n//                     </div>\n//                     <div className=\"archive\" id=\"archive\">\n//                         <h2>Archive</h2>\n//                         <p>Access archives of past talks and events.</p>\n//                     </div>\n//                 </section>\n//             </main>\n//             <footer>\n//                 <p>&copy; 2024 Binghamton University State University of New York</p>\n//             </footer>\n//         </div>\n//     );\n// }\n\n// export default App;\n\nimport React from 'react';\nimport './App.css';\nimport Header from './components/Header';\nimport TalksSection from './components/TalksSection';\nimport Footer from './components/Footer';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nfunction App() {\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"App\",\n    children: [/*#__PURE__*/_jsxDEV(Header, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 129,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"main\", {\n      children: /*#__PURE__*/_jsxDEV(TalksSection, {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 131,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 130,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(Footer, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 133,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 128,\n    columnNumber: 5\n  }, this);\n}\n_c = App;\nexport default App;\nvar _c;\n$RefreshReg$(_c, \"App\");","map":{"version":3,"names":["React","Header","TalksSection","Footer","jsxDEV","_jsxDEV","App","className","children","fileName","_jsxFileName","lineNumber","columnNumber","_c","$RefreshReg$"],"sources":["D:/Binghamton/bing-robotics.github.io/src/App.js"],"sourcesContent":["// import React from 'react';\n// import './App.css';\n// import logo from './assets/logo1.jpeg';\n\n// function App() {\n//     return (\n//         <div className=\"App\">\n//             <header className=\"App-header\">\n//                 <img src={logo} className=\"App-logo\" alt=\"Binghamton University Logo\" />\n//                 <h1>Binghamton Robotics Seminar Talks</h1>\n//             </header>\n//             <header className=\"App-navbar\">\n//                 <nav>\n//                     <ul>\n//                         <li><a href=\"#upcoming-talks\">UPCOMING TALKS</a></li>\n//                         <li><a href=\"#past-talks\">PAST TALKS</a></li>\n//                         <li><a href=\"#archive\">ARCHIVE</a></li>\n//                     </ul>\n//                 </nav>\n//             </header>\n//             <main>\n//             <section className=\"talks-section\">\n//                     <div className=\"upcoming-talks\" id=\"upcoming-talks\">\n//                         <h2>Upcoming Talks</h2>\n//                         {/* New Talk Entry for Shiming Fang*/}\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p> Wednesday, October 16, 2024, 11:00 AM, EB P03 |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>Shiming Fang</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial   guesses,   labor-intensive  fitting  procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges,   we introduce the Fine-Tuning Hybrid Dynamics   (FTHD)   method,   which   integrates   supervised   and unsupervised   Physics-Informed   Neural   Networks   (PINNs), combining  physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively   manage   noisy   real-world   data,   ensuring   accurate denoising   while   preserving   the   vehicle's   essential   physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale   real-world   experiments   from   the   Indy   Autonomous Challenge.   Results   demonstrate   that   the   hybrid   approach significantly improves parameter estimation accuracy, even with reduced   data,   and   outperforms   existing   models.   EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.</p>\n//                                     <p><strong>Bio:</strong> Shiming Fang received the B.S. degree in Mechanical Engineering from Wuhan University of Technology, Wuhan, China in 2016, and the M.S. degree in Mechanical Engineering from University of Birmingham, Birmingham, UK in 2017. He is currently working towards a Ph.D. degree in Mechanical Engineering at Binghamton University. His current research focuses on autonomous vehicle modeling, robust control, and artificial intelligence in autonomous driving.</p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                         {/* New Talk Entry for Stephanie Jesso*/}\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p> Wednesday, October 23, 2024, 11:00 AM, EB P03 |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>Stephanie Jesso</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Historically, the fields of computer science, cognitive science, and neuroscience have been tightly linked. To date, this collaboration has yielded major advances in how the brain and mind are understood, as well as the ways in which artificial minds can be constructed to serve as new collaborators to humans. Yet there are still significant gaps between the capabilities of state-of-the-art autonomous robots and the expectations developed by real users who are now encountering autonomous robots on the job. Human-centered design and human factors research can help to bridge these gaps to create safe, effective, and desirable human-robot systems.</p>\n//                                     <p>In this talk, I will discuss how human minds compare to current-state AI systems, and how insights from human cognition can be used to create more efficient and capable autonomous robots. I will also present a case study of an evaluation conducted on two autonomous robots intended to aid nurses within hospital settings: Moxi and TUG. Both cobots were originally considered for procurement by our collaborating healthcare system, UHS, at which time our team discovered a lack of evidence on either platform in academic literature. This led us to analyze user comments on social media, which elucidated common struggles within real-world environments. In order to improve the fit of autonomous robots into human environments, collaborative research must be conducted and evidence must be shared. Hope to see you there! </p>\n//                                     <p><strong>Relevant Papers:</strong></p>\n//                                     <p><a href=\"https://www.researchgate.net/publication/380014080_On_the_potential_for_human-centered_cognitively_inspired_AI_to_bridge_the_gap_between_optimism_and_reality_for_autonomous_robotics_in_healthcare_a_respectful_critique\"> Feel free to check out our conference proceedings paper</a>!</p>\n//                                     <p><strong>Bio:</strong> Dr. Stephanie Tulk Jesso is an assistant professor in the School of Systems Science and Industrial Engineering. She is the PI of the Human-Centered Mindful Technologies Lab at Binghamton, a co-founder of the SUNY Nursing, Engineering and Applied Research Collaborative, and focuses on research related to design, implementation, and impacts of emerging technologies in the real world.</p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                     </div>\n//                     <div className=\"past-talks\" id=\"past-talks\">\n//                         <h2>Past Talks</h2>\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p> Friday, September 13, 2024, 12:00 PM, Engineering Building, T1 |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>Zain Nasir</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Mapping and Localization in large environments is becoming increasingly important for autonomous UAV swarms. UAV swarms solving problems in disaster response, infrastructure inspection, and agriculture rely on fresh and accurate maps to make navigation decisions. SLAM methods are capable of providing highly accurate maps through visual information, but are computationally heavy and ill-suited for unmanned aerial vehicles. For this reason, UAV swarms often dedicate one or more drones to frequent mapping, while other drones use map information for planning and trajectory generation. UAV swarms also centralize heavy-weight workloads like AI inference and SLAM map combination at the edge to extend UAV battery lives at the expense of network availability. Both map sharing and offloading necessitate high network bandwidth, but few SLAM or planning approaches account for this. We present NetSLAM, a network-assisted SLAM and planning system that builds environmental maps and UAV trajectories that meet quality of service requirements. NetSLAM embeds network information into SLAM maps so planning can compensate for changes in network connectivity across the environment. We also present NetPlan, a path planning algorithm which utilizes NetSLAM maps to build trajectories that maintain network connectivity requirements to maximize performance. Through real-world experiments and simulation, we show that NetSLAM maps network environments with limited additional overhead compared to existing SLAM approaches, while improving offloading performance significantly when multi-agent swarms consider network availability.</p>\n//                                     <p><strong>Bio:</strong> I am an incoming software developer and researcher at Memorial Sloan Kettering currently completing my Masters in Computer Science (AI Track) from Binghamton University. My expertise includes autonomous navigation systems in drones, computer vision, and machine learning. My current research involves optimizing edge-based SLAM techniques and tailoring them for deployment on small drones with limited compute power.</p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                         <div className=\"talk-notes\">\n//                             <p>This Bing-Robotics talk is also part of the SoC Seminar Series, jointly hosted by Profs. KD Kang and William Hallahan.</p>\n//                         </div>\n//                         {/* New Talk Entry for David DeFazio */}\n//                         <div className=\"talk\">\n//                             <div className=\"talk-details\">\n//                                 <div className=\"talk-time\">\n//                                     <p>Friday, August 30, 2024, 12:00 PM, Fountain Room (Smart Energy Building, ITC), Side A |</p>\n//                                 </div>\n//                                 <div className=\"talk-abstract\">\n//                                     <h3>David DeFazio</h3>\n//                                     <p>\n//                                         <strong>Title:</strong> \n//                                         <span className=\"normal-text\"> Legged Locomotion and Collaborative Decision Making in Human-Robot Teams</span>\n//                                     </p>\n//                                     <p><strong>Abstract:</strong> Legged robots are of great interest to the robotics community, due to their capacity for agile movements and robust locomotion in challenging environments. Several methods exist to enable locomotion and navigation capabilities for legged robots, however these methods generally do not leverage human domain knowledge or support direct interactions with people for collaborative decision making. In this talk, I will present a human-in-the-loop approach for both locomotion gait design and guided navigation. Each approach has been deployed on hardware, showcasing diverse locomotion gaits, and guided navigation for the visually impaired.</p>\n//                                     <p><strong>Bio:</strong> </p>\n//                                 </div>\n//                             </div>\n//                         </div>\n//                         <div className=\"talk-notes\">\n//                             <p>Prof. Kaiyan Yu will offer a lab tour for all who are interested after the seminar talk.</p>\n//                         </div>\n//                     </div>\n//                     <div className=\"archive\" id=\"archive\">\n//                         <h2>Archive</h2>\n//                         <p>Access archives of past talks and events.</p>\n//                     </div>\n//                 </section>\n//             </main>\n//             <footer>\n//                 <p>&copy; 2024 Binghamton University State University of New York</p>\n//             </footer>\n//         </div>\n//     );\n// }\n\n// export default App;\n\nimport React from 'react';\nimport './App.css';\nimport Header from './components/Header';\nimport TalksSection from './components/TalksSection';\nimport Footer from './components/Footer';\n\nfunction App() {\n  return (\n    <div className=\"App\">\n      <Header />\n      <main>\n        <TalksSection />\n      </main>\n      <Footer />\n    </div>\n  );\n}\n\nexport default App;\n"],"mappings":";AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA,OAAOA,KAAK,MAAM,OAAO;AACzB,OAAO,WAAW;AAClB,OAAOC,MAAM,MAAM,qBAAqB;AACxC,OAAOC,YAAY,MAAM,2BAA2B;AACpD,OAAOC,MAAM,MAAM,qBAAqB;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAEzC,SAASC,GAAGA,CAAA,EAAG;EACb,oBACED,OAAA;IAAKE,SAAS,EAAC,KAAK;IAAAC,QAAA,gBAClBH,OAAA,CAACJ,MAAM;MAAAQ,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAE,CAAC,eACVP,OAAA;MAAAG,QAAA,eACEH,OAAA,CAACH,YAAY;QAAAO,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAE;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACZ,CAAC,eACPP,OAAA,CAACF,MAAM;MAAAM,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAE,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACP,CAAC;AAEV;AAACC,EAAA,GAVQP,GAAG;AAYZ,eAAeA,GAAG;AAAC,IAAAO,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}
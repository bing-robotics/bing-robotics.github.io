{"ast":null,"code":"import React from\"react\";import Talk from\"./Talk\";import{jsx as _jsx,jsxs as _jsxs}from\"react/jsx-runtime\";function TalksSection(){return/*#__PURE__*/_jsxs(\"section\",{className:\"talks-section\",children:[/*#__PURE__*/_jsxs(\"div\",{id:\"talks-schedule\",className:\"talks-wrapper\",children:[/*#__PURE__*/_jsx(\"h2\",{children:\"Robotics Talk Schedule 2025-2026\"}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#Legged Locomotion and Collaborative Decision Making in Human-Robot Teams\",children:\"August 30, 2024: David DeFazio\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms\",children:\"September 13, 2024: Zain Nasir\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation\",children:\"October 16, 2024: Shiming Fang\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique\",children:\"October 23, 2024: Dr. Stephanie Tulk Jesso\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#Autonomously Learning World-Model Representations For Efficient Robot Planning\",children:\"November 15, 2024: Naman Shah\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#Planning with Foundation Models: From Service to Assistive Robotics\",children:\"September 03, 2025: Shiqi Zhang\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{fontSize:\"18px\"},children:/*#__PURE__*/_jsx(\"ul\",{children:/*#__PURE__*/_jsx(\"li\",{children:/*#__PURE__*/_jsx(\"a\",{href:\"#Real-time Hyperspectral Image Processing for Small UAVs\",children:\"September 24, 2025: Jayson Boubin\"})})})}),/*#__PURE__*/_jsx(\"p\",{style:{marginTop:\"30px\",fontSize:\"18px\"},children:\"More to be announced!\"})]}),/*#__PURE__*/_jsxs(\"div\",{id:\"upcoming-talks\",className:\"talks-wrapper\",children:[/*#__PURE__*/_jsx(\"h2\",{children:\"Upcoming Talks 2025-2026\"}),/*#__PURE__*/_jsx(Talk,{id:\"Real-time Hyperspectral Image Processing for Small UAVs\",time:\"Wednesday, September 24, 2025, 02:00 PM, EB T1\",speaker:\"Jayson Boubin\",title:\"Real-time Hyperspectral Image Processing for Small UAVs\",abstract:\"Unmanned aerial vehicles (UAV) have emerged in recent years as powerful, maneuverable sensors capable of real-time computer vision. Real-time image processing onboard UAV often requires data or model compression, acceleration, or edge offloading and is generally restricted to conventional RGB cameras. In this study, we consider real-time in-situ processing for hyperspectral imaging (HSI). HSI cameras detect many wavelengths of light. Material-specific spectral signatures can be matched to camera outputs to identify materials in a UAV\\u2019s environment, but HSI cameras produce large amounts of information that generally require offline processing by heavy-weight software. We present REMIX, a real-time hyperspectral processing payload for small UAV. REMIX uses a custom software library, light-weight hyperspectral camera, and small embedded device to process and visualize HSI data in real-time. REMIX processes HSI lines in under 5ms, allowing HSI perception to be visualized in real-time where conventional methods may take hours. We show that, when properly configured, adding real-time processing via REMIX degrades UAV flight time by only 4% and increases HSI processing speeds by up to 6X compared to naive payloads, and further decreases post-processing time by 20.48X compared to conventional methods, even when using significantly less powerful equipment.\"})]}),/*#__PURE__*/_jsxs(\"div\",{id:\"past-talks\",className:\"talks-wrapper\",children:[/*#__PURE__*/_jsx(\"h2\",{children:\"Past Talks 2025-2026\"}),/*#__PURE__*/_jsx(Talk,{id:\"Planning with Foundation Models: From Service to Assistive Robotics\",time:\"Wednesday, September 03, 2025, 02:00 PM, EB T1\",speaker:\"Shiqi Zhang\",title:\"Planning with Foundation Models: From Service to Assistive Robotics\",abstract:\"Robots need task planning algorithms for sequencing high-level actions, and motion planning algorithms for computing continuous trajectories for realizing those task-level actions. Task and motion planning (TAMP) algorithms are for interleaving those two types of planning paradigms to ensure task completions. The real world is generally \\u201Copen\\u201D with new objects and unforeseen situations. In this talk, I\\u2019ll present our recent efforts on applying foundation models and augmented reality to human-robot planning systems. We will discuss application domains from service robots setting up tables and object delivery to robot dogs assisting people with visual impairment in navigation\",bio:\"Dr. Shiqi Zhang is an Associate Professor with the School of Computing, the State University of New York at Binghamton (SUNY Binghamton). His research interests include robot decision making, robot learning and human-robot systems. He was a Postdoc at the University of Texas at Austin and received his Ph.D. in Computer Science from Texas Tech University. His research has been supported by NSF, OPPO (Faculty Research Award), Ford (URP Award), DEEP Robotics and Guiding Eyes. He received the Best Robotics Paper Award from the 2018 AAMAS conference, Outstanding Associate Editor recognition in 2024 from the IEEE RA-L journal.\"}),/*#__PURE__*/_jsx(\"h2\",{children:\"Past Talks 2024-2025\"}),/*#__PURE__*/_jsx(Talk,{id:\"Autonomously Learning World-Model Representations For Efficient Robot Planning\",time:\"Wednesday, November 15, 2024, 12:00 PM, EB T1\",speaker:\"Naman Shah\",title:\"Autonomously Learning World-Model Representations For Efficient Robot Planning\",abstract:\"In recent years, it has been clear that planning is an essential tool for robots to achieve complex goals. However, robots often heavily rely on humans to provide 'world models' that enable long-horizon planning. It is not only expensive to create such world models as it requires human experts who understand the domains as well as limitations of the robot, but these human-generated world models are often biased by human intuition and kinematic constraints. In this talk, I will present my research focusing on autonomously learning plannable world models. The talk would involve discussing approaches on task and motion planning, neuro-symbolic abstractions for motion planning, and how we can learn world models for task and motion planning.\",bio:\"Naman is a Postdoctoral researcher in Intelligent Robots Lab (IRL) with Prof. George Konidaris. He has completed his PhD from Arizona State University supervised by Prof. Siddharth Srivastava. His research interest lies in investigating methods for autonomously inventing generalizable and plannable world models for robotics tasks. He has been an intern with Palo Alto Research Center, Amazon Robotics, and Toyota Research Institute. Naman has also achieved several graduate fellowships at ASU and a Best Demo Paper Award at AAMAS 2022.\",cohost:\"SoC Seminar\"}),/*#__PURE__*/_jsx(Talk,{id:\"On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique\",time:\"Wednesday, October 23, 2024, 11:00 AM, EB P03\",speaker:\"Dr. Stephanie Tulk Jesso\",title:\"On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique\",abstract:\"Historically, the fields of computer science, cognitive science, and neuroscience have been tightly linked. To date, this collaboration has yielded major advances in how the brain and mind are understood, as well as the ways in which artificial minds can be constructed to serve as new collaborators to humans. Yet there are still significant gaps between the capabilities of state-of-the-art autonomous robots and the expectations developed by real users who are now encountering autonomous robots on the job. Human-centered design and human factors research can help to bridge these gaps to create safe, effective, and desirable human-robot systems.\",abstract1:\"In this talk, I will discuss how human minds compare to current-state AI systems, and how insights from human cognition can be used to create more efficient and capable autonomous robots. I will also present a case study of an evaluation conducted on two autonomous robots intended to aid nurses within hospital settings: Moxi and TUG. Both cobots were originally considered for procurement by our collaborating healthcare system, UHS, at which time our team discovered a lack of evidence on either platform in academic literature. This led us to analyze user comments on social media, which elucidated common struggles within real-world environments. In order to improve the fit of autonomous robots into human environments, collaborative research must be conducted and evidence must be shared. Hope to see you there!\",papers:/*#__PURE__*/_jsxs(\"a\",{href:\"https://www.researchgate.net/publication/380014080_On_the_potential_for_human-centered_cognitively_inspired_AI_to_bridge_the_gap_between_optimism_and_reality_for_autonomous_robotics_in_healthcare_a_respectful_critique\",target:\"_blank\",rel:\"noopener noreferrer\",children:[\" \",\"Feel free to check out our conference proceedings paper here!\"]}),bio:\"Dr. Stephanie Tulk Jesso is an assistant professor in the School of Systems Science and Industrial Engineering. She is the PI of the Human-Centered Mindful Technologies Lab at Binghamton, a co-founder of the SUNY Nursing, Engineering and Applied Research Collaborative, and focuses on research related to design, implementation, and impacts of emerging technologies in the real world.\"}),/*#__PURE__*/_jsx(Talk,{id:\"Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation\",time:\"Wednesday, October 16, 2024, 11:00 AM, EB R15\",speaker:\"Shiming Fang\",title:\"Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation\",abstract:\"Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial   guesses,   labor-intensive  fitting  procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges,   we introduce the Fine-Tuning Hybrid Dynamics   (FTHD)   method,   which   integrates   supervised   and unsupervised   Physics-Informed   Neural   Networks   (PINNs), combining  physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively   manage   noisy   real-world   data,   ensuring   accurate denoising   while   preserving   the   vehicle's   essential   physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale   real-world   experiments   from   the   Indy   Autonomous Challenge.   Results   demonstrate   that   the   hybrid   approach significantly improves parameter estimation accuracy, even with reduced   data,   and   outperforms   existing   models.   EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.\",bio:\"Shiming Fang received the B.S. degree in Mechanical Engineering from Wuhan University of Technology, Wuhan, China in 2016, and the M.S. degree in Mechanical Engineering from University of Birmingham, Birmingham, UK in 2017. He is currently working towards a Ph.D. degree in Mechanical Engineering at Binghamton University. His current research focuses on autonomous vehicle modeling, robust control, and artificial intelligence in autonomous driving.\"}),/*#__PURE__*/_jsx(Talk,{id:\"NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms\",time:\"Friday, September 13, 2024, 12:00 PM, Engineering Building, T1 \",speaker:\"Zain Nasir\",title:\"NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms\",abstract:\"Mapping and Localization in large environments is becoming increasingly important for autonomous UAV swarms. UAV swarms solving problems in disaster response, infrastructure inspection, and agriculture rely on fresh and accurate maps to make navigation decisions. SLAM methods are capable of providing highly accurate maps through visual information, but are computationally heavy and ill-suited for unmanned aerial vehicles. For this reason, UAV swarms often dedicate one or more drones to frequent mapping, while other drones use map information for planning and trajectory generation. UAV swarms also centralize heavy-weight workloads like AI inference and SLAM map combination at the edge to extend UAV battery lives at the expense of network availability. Both map sharing and offloading necessitate high network bandwidth, but few SLAM or planning approaches account for this. We present NetSLAM, a network-assisted SLAM and planning system that builds environmental maps and UAV trajectories that meet quality of service requirements. NetSLAM embeds network information into SLAM maps so planning can compensate for changes in network connectivity across the environment. We also present NetPlan, a path planning algorithm which utilizes NetSLAM maps to build trajectories that maintain network connectivity requirements to maximize performance. Through real-world experiments and simulation, we show that NetSLAM maps network environments with limited additional overhead compared to existing SLAM approaches, while improving offloading performance significantly when multi-agent swarms consider network availability.\",bio:\"I am an incoming software developer and researcher at Memorial Sloan Kettering currently completing my Masters in Computer Science (AI Track) from Binghamton University. My expertise includes autonomous navigation systems in drones, computer vision, and machine learning. My current research involves optimizing edge-based SLAM techniques and tailoring them for deployment on small drones with limited compute power.\",cohost:\"Prof.K D Kang & Prof.William Hallahan\"}),/*#__PURE__*/_jsx(Talk,{id:\"Legged Locomotion and Collaborative Decision Making in Human-Robot Teams\",time:\"Friday, August 30, 2024, 12:00 PM, Fountain Room (Smart Energy Building, ITC), Side A\",speaker:\"David DeFazio\",title:\"Legged Locomotion and Collaborative Decision Making in Human-Robot Teams\",abstract:\"Legged robots are of great interest to the robotics community, due to their capacity for agile movements and robust locomotion in challenging environments. Several methods exist to enable locomotion and navigation capabilities for legged robots, however these methods generally do not leverage human domain knowledge or support direct interactions with people for collaborative decision making. In this talk, I will present a human-in-the-loop approach for both locomotion gait design and guided navigation. Each approach has been deployed on hardware, showcasing diverse locomotion gaits, and guided navigation for the visually impaired.\",bio:\"\",notes:\"Prof. Kaiyan Yu will offer a lab tour for all who are interested after the seminar talk.\"})]}),/*#__PURE__*/_jsx(\"div\",{id:\"archive\",className:\"talks-wrapper\",children:/*#__PURE__*/_jsx(\"h2\",{children:\"Archive\"})})]});}export default TalksSection;","map":{"version":3,"names":["React","Talk","jsx","_jsx","jsxs","_jsxs","TalksSection","className","children","id","style","fontSize","href","marginTop","time","speaker","title","abstract","bio","cohost","abstract1","papers","target","rel","notes"],"sources":["/Users/sahilbhosale/Developer/Bing-Robotics/bing-robotics.github.io/src/components/TalksSection.js"],"sourcesContent":["import React from \"react\";\nimport Talk from \"./Talk\";\n\nfunction TalksSection() {\n  return (\n    <section className=\"talks-section\">\n      <div id=\"talks-schedule\" className=\"talks-wrapper\">\n        <h2>Robotics Talk Schedule 2025-2026</h2>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#Legged Locomotion and Collaborative Decision Making in Human-Robot Teams\">\n                August 30, 2024: David DeFazio\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms\">\n                September 13, 2024: Zain Nasir\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation\">\n                October 16, 2024: Shiming Fang\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique\">\n                October 23, 2024: Dr. Stephanie Tulk Jesso\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#Autonomously Learning World-Model Representations For Efficient Robot Planning\">\n                November 15, 2024: Naman Shah\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#Planning with Foundation Models: From Service to Assistive Robotics\">\n                September 03, 2025: Shiqi Zhang\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ fontSize: \"18px\" }}>\n          <ul>\n            <li>\n              <a href=\"#Real-time Hyperspectral Image Processing for Small UAVs\">\n                September 24, 2025: Jayson Boubin\n              </a>\n            </li>\n          </ul>\n        </p>\n        <p style={{ marginTop: \"30px\", fontSize: \"18px\" }}>\n          More to be announced!\n        </p>\n      </div>\n      <div id=\"upcoming-talks\" className=\"talks-wrapper\">\n        <h2>Upcoming Talks 2025-2026</h2>\n        <Talk\n          id=\"Real-time Hyperspectral Image Processing for Small UAVs\"\n          time=\"Wednesday, September 24, 2025, 02:00 PM, EB T1\"\n          speaker=\"Jayson Boubin\"\n          title=\"Real-time Hyperspectral Image Processing for Small UAVs\"\n          abstract=\"Unmanned aerial vehicles (UAV) have emerged in recent years as powerful, maneuverable sensors capable of real-time computer vision. Real-time image processing onboard UAV often requires data or model compression, acceleration, or edge offloading and is generally restricted to conventional RGB cameras. In this study, we consider real-time in-situ processing for hyperspectral imaging (HSI). HSI cameras detect many wavelengths of light. Material-specific spectral signatures can be matched to camera outputs to identify materials in a UAV’s environment, but HSI cameras produce large amounts of information that generally require offline processing by heavy-weight software. We present REMIX, a real-time hyperspectral processing payload for small UAV. REMIX uses a custom software library, light-weight hyperspectral camera, and small embedded device to process and visualize HSI data in real-time. REMIX processes HSI lines in under 5ms, allowing HSI perception to be visualized in real-time where conventional methods may take hours. We show that, when properly configured, adding real-time processing via REMIX degrades UAV flight time by only 4% and increases HSI processing speeds by up to 6X compared to naive payloads, and further decreases post-processing time by 20.48X compared to conventional methods, even when using significantly less powerful equipment.\"\n        />\n      </div>\n      <div id=\"past-talks\" className=\"talks-wrapper\">\n        <h2>Past Talks 2025-2026</h2>\n        <Talk\n          id=\"Planning with Foundation Models: From Service to Assistive Robotics\"\n          time=\"Wednesday, September 03, 2025, 02:00 PM, EB T1\"\n          speaker=\"Shiqi Zhang\"\n          title=\"Planning with Foundation Models: From Service to Assistive Robotics\"\n          abstract=\"Robots need task planning algorithms for sequencing high-level actions, and motion planning algorithms for computing continuous trajectories for realizing those task-level actions. Task and motion planning (TAMP) algorithms are for interleaving those two types of planning paradigms to ensure task completions. The real world is generally “open” with new objects and unforeseen situations. In this talk, I’ll present our recent efforts on applying foundation models and augmented reality to human-robot planning systems. We will discuss application domains from service robots setting up tables and object delivery to robot dogs assisting people with visual impairment in navigation\"\n          bio=\"Dr. Shiqi Zhang is an Associate Professor with the School of Computing, the State University of New York at Binghamton (SUNY Binghamton). His research interests include robot decision making, robot learning and human-robot systems. He was a Postdoc at the University of Texas at Austin and received his Ph.D. in Computer Science from Texas Tech University. His research has been supported by NSF, OPPO (Faculty Research Award), Ford (URP Award), DEEP Robotics and Guiding Eyes. He received the Best Robotics Paper Award from the 2018 AAMAS conference, Outstanding Associate Editor recognition in 2024 from the IEEE RA-L journal.\"\n        />\n        <h2>Past Talks 2024-2025</h2>\n        <Talk\n          id=\"Autonomously Learning World-Model Representations For Efficient Robot Planning\"\n          time=\"Wednesday, November 15, 2024, 12:00 PM, EB T1\"\n          speaker=\"Naman Shah\"\n          title=\"Autonomously Learning World-Model Representations For Efficient Robot Planning\"\n          abstract=\"In recent years, it has been clear that planning is an essential tool for robots to achieve complex goals. However, robots often heavily rely on humans to provide 'world models' that enable long-horizon planning. It is not only expensive to create such world models as it requires human experts who understand the domains as well as limitations of the robot, but these human-generated world models are often biased by human intuition and kinematic constraints. In this talk, I will present my research focusing on autonomously learning plannable world models. The talk would involve discussing approaches on task and motion planning, neuro-symbolic abstractions for motion planning, and how we can learn world models for task and motion planning.\"\n          bio=\"Naman is a Postdoctoral researcher in Intelligent Robots Lab (IRL) with Prof. George Konidaris. He has completed his PhD from Arizona State University supervised by Prof. Siddharth Srivastava. His research interest lies in investigating methods for autonomously inventing generalizable and plannable world models for robotics tasks. He has been an intern with Palo Alto Research Center, Amazon Robotics, and Toyota Research Institute. Naman has also achieved several graduate fellowships at ASU and a Best Demo Paper Award at AAMAS 2022.\"\n          cohost=\"SoC Seminar\"\n        />\n        <Talk\n          id=\"On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique\"\n          time=\"Wednesday, October 23, 2024, 11:00 AM, EB P03\"\n          speaker=\"Dr. Stephanie Tulk Jesso\"\n          title=\"On the potential for human-centered, cognitively inspired AI for autonomous robotics in healthcare: a respectful critique\"\n          abstract=\"Historically, the fields of computer science, cognitive science, and neuroscience have been tightly linked. To date, this collaboration has yielded major advances in how the brain and mind are understood, as well as the ways in which artificial minds can be constructed to serve as new collaborators to humans. Yet there are still significant gaps between the capabilities of state-of-the-art autonomous robots and the expectations developed by real users who are now encountering autonomous robots on the job. Human-centered design and human factors research can help to bridge these gaps to create safe, effective, and desirable human-robot systems.\"\n          abstract1=\"In this talk, I will discuss how human minds compare to current-state AI systems, and how insights from human cognition can be used to create more efficient and capable autonomous robots. I will also present a case study of an evaluation conducted on two autonomous robots intended to aid nurses within hospital settings: Moxi and TUG. Both cobots were originally considered for procurement by our collaborating healthcare system, UHS, at which time our team discovered a lack of evidence on either platform in academic literature. This led us to analyze user comments on social media, which elucidated common struggles within real-world environments. In order to improve the fit of autonomous robots into human environments, collaborative research must be conducted and evidence must be shared. Hope to see you there!\"\n          papers={\n            <a\n              href=\"https://www.researchgate.net/publication/380014080_On_the_potential_for_human-centered_cognitively_inspired_AI_to_bridge_the_gap_between_optimism_and_reality_for_autonomous_robotics_in_healthcare_a_respectful_critique\"\n              target=\"_blank\"\n              rel=\"noopener noreferrer\"\n            >\n              {\" \"}\n              Feel free to check out our conference proceedings paper here!\n            </a>\n          }\n          bio=\"Dr. Stephanie Tulk Jesso is an assistant professor in the School of Systems Science and Industrial Engineering. She is the PI of the Human-Centered Mindful Technologies Lab at Binghamton, a co-founder of the SUNY Nursing, Engineering and Applied Research Collaborative, and focuses on research related to design, implementation, and impacts of emerging technologies in the real world.\"\n        />\n        <Talk\n          id=\"Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation\"\n          time=\"Wednesday, October 16, 2024, 11:00 AM, EB R15\"\n          speaker=\"Shiming Fang\"\n          title=\"Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation\"\n          abstract=\"Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial   guesses,   labor-intensive  fitting  procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges,   we introduce the Fine-Tuning Hybrid Dynamics   (FTHD)   method,   which   integrates   supervised   and unsupervised   Physics-Informed   Neural   Networks   (PINNs), combining  physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively   manage   noisy   real-world   data,   ensuring   accurate denoising   while   preserving   the   vehicle's   essential   physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale   real-world   experiments   from   the   Indy   Autonomous Challenge.   Results   demonstrate   that   the   hybrid   approach significantly improves parameter estimation accuracy, even with reduced   data,   and   outperforms   existing   models.   EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.\"\n          bio=\"Shiming Fang received the B.S. degree in Mechanical Engineering from Wuhan University of Technology, Wuhan, China in 2016, and the M.S. degree in Mechanical Engineering from University of Birmingham, Birmingham, UK in 2017. He is currently working towards a Ph.D. degree in Mechanical Engineering at Binghamton University. His current research focuses on autonomous vehicle modeling, robust control, and artificial intelligence in autonomous driving.\"\n        />\n        <Talk\n          id=\"NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms\"\n          time=\"Friday, September 13, 2024, 12:00 PM, Engineering Building, T1 \"\n          speaker=\"Zain Nasir\"\n          title=\"NetSLAM: Network-Aware Path Planning for Edge-Assisted UAV Swarms\"\n          abstract=\"Mapping and Localization in large environments is becoming increasingly important for autonomous UAV swarms. UAV swarms solving problems in disaster response, infrastructure inspection, and agriculture rely on fresh and accurate maps to make navigation decisions. SLAM methods are capable of providing highly accurate maps through visual information, but are computationally heavy and ill-suited for unmanned aerial vehicles. For this reason, UAV swarms often dedicate one or more drones to frequent mapping, while other drones use map information for planning and trajectory generation. UAV swarms also centralize heavy-weight workloads like AI inference and SLAM map combination at the edge to extend UAV battery lives at the expense of network availability. Both map sharing and offloading necessitate high network bandwidth, but few SLAM or planning approaches account for this. We present NetSLAM, a network-assisted SLAM and planning system that builds environmental maps and UAV trajectories that meet quality of service requirements. NetSLAM embeds network information into SLAM maps so planning can compensate for changes in network connectivity across the environment. We also present NetPlan, a path planning algorithm which utilizes NetSLAM maps to build trajectories that maintain network connectivity requirements to maximize performance. Through real-world experiments and simulation, we show that NetSLAM maps network environments with limited additional overhead compared to existing SLAM approaches, while improving offloading performance significantly when multi-agent swarms consider network availability.\"\n          bio=\"I am an incoming software developer and researcher at Memorial Sloan Kettering currently completing my Masters in Computer Science (AI Track) from Binghamton University. My expertise includes autonomous navigation systems in drones, computer vision, and machine learning. My current research involves optimizing edge-based SLAM techniques and tailoring them for deployment on small drones with limited compute power.\"\n          cohost=\"Prof.K D Kang & Prof.William Hallahan\"\n        />\n        <Talk\n          id=\"Legged Locomotion and Collaborative Decision Making in Human-Robot Teams\"\n          time=\"Friday, August 30, 2024, 12:00 PM, Fountain Room (Smart Energy Building, ITC), Side A\"\n          speaker=\"David DeFazio\"\n          title=\"Legged Locomotion and Collaborative Decision Making in Human-Robot Teams\"\n          abstract=\"Legged robots are of great interest to the robotics community, due to their capacity for agile movements and robust locomotion in challenging environments. Several methods exist to enable locomotion and navigation capabilities for legged robots, however these methods generally do not leverage human domain knowledge or support direct interactions with people for collaborative decision making. In this talk, I will present a human-in-the-loop approach for both locomotion gait design and guided navigation. Each approach has been deployed on hardware, showcasing diverse locomotion gaits, and guided navigation for the visually impaired.\"\n          bio=\"\"\n          notes=\"Prof. Kaiyan Yu will offer a lab tour for all who are interested after the seminar talk.\"\n        />\n      </div>\n      <div id=\"archive\" className=\"talks-wrapper\">\n        <h2>Archive</h2>\n      </div>\n    </section>\n  );\n}\n\nexport default TalksSection;\n"],"mappings":"AAAA,MAAO,CAAAA,KAAK,KAAM,OAAO,CACzB,MAAO,CAAAC,IAAI,KAAM,QAAQ,CAAC,OAAAC,GAAA,IAAAC,IAAA,CAAAC,IAAA,IAAAC,KAAA,yBAE1B,QAAS,CAAAC,YAAYA,CAAA,CAAG,CACtB,mBACED,KAAA,YAASE,SAAS,CAAC,eAAe,CAAAC,QAAA,eAChCH,KAAA,QAAKI,EAAE,CAAC,gBAAgB,CAACF,SAAS,CAAC,eAAe,CAAAC,QAAA,eAChDL,IAAA,OAAAK,QAAA,CAAI,kCAAgC,CAAI,CAAC,cACzCL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,2EAA2E,CAAAJ,QAAA,CAAC,gCAEpF,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,oEAAoE,CAAAJ,QAAA,CAAC,gCAE7E,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,4FAA4F,CAAAJ,QAAA,CAAC,gCAErG,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,4HAA4H,CAAAJ,QAAA,CAAC,4CAErI,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,iFAAiF,CAAAJ,QAAA,CAAC,+BAE1F,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,sEAAsE,CAAAJ,QAAA,CAAC,iCAE/E,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEC,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,cAC7BL,IAAA,OAAAK,QAAA,cACEL,IAAA,OAAAK,QAAA,cACEL,IAAA,MAAGS,IAAI,CAAC,0DAA0D,CAAAJ,QAAA,CAAC,mCAEnE,CAAG,CAAC,CACF,CAAC,CACH,CAAC,CACJ,CAAC,cACJL,IAAA,MAAGO,KAAK,CAAE,CAAEG,SAAS,CAAE,MAAM,CAAEF,QAAQ,CAAE,MAAO,CAAE,CAAAH,QAAA,CAAC,uBAEnD,CAAG,CAAC,EACD,CAAC,cACNH,KAAA,QAAKI,EAAE,CAAC,gBAAgB,CAACF,SAAS,CAAC,eAAe,CAAAC,QAAA,eAChDL,IAAA,OAAAK,QAAA,CAAI,0BAAwB,CAAI,CAAC,cACjCL,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,yDAAyD,CAC5DK,IAAI,CAAC,gDAAgD,CACrDC,OAAO,CAAC,eAAe,CACvBC,KAAK,CAAC,yDAAyD,CAC/DC,QAAQ,CAAC,g2CAA21C,CACr2C,CAAC,EACC,CAAC,cACNZ,KAAA,QAAKI,EAAE,CAAC,YAAY,CAACF,SAAS,CAAC,eAAe,CAAAC,QAAA,eAC5CL,IAAA,OAAAK,QAAA,CAAI,sBAAoB,CAAI,CAAC,cAC7BL,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,qEAAqE,CACxEK,IAAI,CAAC,gDAAgD,CACrDC,OAAO,CAAC,aAAa,CACrBC,KAAK,CAAC,qEAAqE,CAC3EC,QAAQ,CAAC,2rBAA4qB,CACrrBC,GAAG,CAAC,snBAAsnB,CAC3nB,CAAC,cACFf,IAAA,OAAAK,QAAA,CAAI,sBAAoB,CAAI,CAAC,cAC7BL,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,gFAAgF,CACnFK,IAAI,CAAC,+CAA+C,CACpDC,OAAO,CAAC,YAAY,CACpBC,KAAK,CAAC,gFAAgF,CACtFC,QAAQ,CAAC,4uBAA4uB,CACrvBC,GAAG,CAAC,2hBAA2hB,CAC/hBC,MAAM,CAAC,aAAa,CACrB,CAAC,cACFhB,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,2HAA2H,CAC9HK,IAAI,CAAC,+CAA+C,CACpDC,OAAO,CAAC,0BAA0B,CAClCC,KAAK,CAAC,2HAA2H,CACjIC,QAAQ,CAAC,6oBAA6oB,CACtpBG,SAAS,CAAC,ozBAAozB,CAC9zBC,MAAM,cACJhB,KAAA,MACEO,IAAI,CAAC,2NAA2N,CAChOU,MAAM,CAAC,QAAQ,CACfC,GAAG,CAAC,qBAAqB,CAAAf,QAAA,EAExB,GAAG,CAAC,+DAEP,EAAG,CACJ,CACDU,GAAG,CAAC,kYAAkY,CACvY,CAAC,cACFf,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,2FAA2F,CAC9FK,IAAI,CAAC,+CAA+C,CACpDC,OAAO,CAAC,cAAc,CACtBC,KAAK,CAAC,2FAA2F,CACjGC,QAAQ,CAAC,iyDAAiyD,CAC1yDC,GAAG,CAAC,ocAAoc,CACzc,CAAC,cACFf,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,mEAAmE,CACtEK,IAAI,CAAC,iEAAiE,CACtEC,OAAO,CAAC,YAAY,CACpBC,KAAK,CAAC,mEAAmE,CACzEC,QAAQ,CAAC,2lDAA2lD,CACpmDC,GAAG,CAAC,kaAAka,CACtaC,MAAM,CAAC,uCAAuC,CAC/C,CAAC,cACFhB,IAAA,CAACF,IAAI,EACHQ,EAAE,CAAC,0EAA0E,CAC7EK,IAAI,CAAC,uFAAuF,CAC5FC,OAAO,CAAC,eAAe,CACvBC,KAAK,CAAC,0EAA0E,CAChFC,QAAQ,CAAC,goBAAgoB,CACzoBC,GAAG,CAAC,EAAE,CACNM,KAAK,CAAC,0FAA0F,CACjG,CAAC,EACC,CAAC,cACNrB,IAAA,QAAKM,EAAE,CAAC,SAAS,CAACF,SAAS,CAAC,eAAe,CAAAC,QAAA,cACzCL,IAAA,OAAAK,QAAA,CAAI,SAAO,CAAI,CAAC,CACb,CAAC,EACC,CAAC,CAEd,CAEA,cAAe,CAAAF,YAAY","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}